\subsection{partial derivatives}

In this section we discuss partial derivatives -- a useful algebraic tool to obtain some geometric properties.
 % TODO

\begin{definition}[partial derivative]
Let $D : k[x_0,..x_n] \to k[x_0,..x_n]$ be a derivation over $k$, i.e. a homomorphism of $k$-modules which satisfies the Leibniz rule $D(xy) = xDy+yDx$ for $x,y \in k[x_0,..x_n]$. Furthermore let $D(h) \in k$ for every linear form $h$. We call such a derivation a \emph{partial derivative}.
\end{definition}

\begin{example}
The standard example for partial derivatives are of course the partial derivates with respect to one of the variables $x_i$, defined as $\del_{x_i}(x_j) = \delta_{i,j} := \begin{cases} 1, & \text{ for } i = j \\ 0, & \text {otherwise.} \end{cases}$
\end{example}

\begin{example}
For any monomial $\{ h_i\}_{i=0}^n$ basis of $\bigoplus_{i=0}^n kx_i \subset k[x_0,..x_n]$ we obtain a family of partial derivatives $\{ \del_{h_i} \}_{i=0}^n$ for which $\del_{h_i}(h_j) = \delta_{i,j}$ holds. The construction goes as follows: Let $M = (a_{i,j})  \in k^{(n+1)\times(n+1)}$ be the base change matrix and $M^{-1} = (\widetilde a_{i,j})$ be its inverse.
This just means $h_i = \sum_{j=0}^n a_{i,j} x_j$ and hence $\del_{x_k}(h_i) = a_{i,k}$.
From $\delta_{i,j} = \sum_{k=0}^n a_{i,k}\widetilde a_{k,j}
= \sum_{k=0}^n \del_{x_k}(h_i) \widetilde a_{k,j}$ it is obvious that we need to define

\begin{equation}
\del_{h_j} = \sum_{k=0}^n \widetilde a_{k,j} \del_{x_k}
\end{equation}

Another way to write this would be
\begin{equation}
\begin{pmatrix} \del_{h_0} \\ \vdots \\ \del_{h_n} \end{pmatrix}
= M^{-T}
\begin{pmatrix} \del_{x_0} \\ \vdots \\ \del_{x_n} \end{pmatrix}
\end{equation}
\end{example}

A useful fact, that allows us to recover a homogeneous polynomial by its partial derivatives is

\begin{proposition}[Euler's formula]
For any $f \in k[x_0,..x_n]$ homogeneous of degree $d$ we have the equality
\[ df = \sum_{i=0}^n \del_{x_i}(f)x_i \]
\end{proposition}
\begin{proof}
By linearity we only need to prove the monomial case $f = \prod_{i=0}^n x_i^{a_i}$, $a_i$ being integers such that $\sum_{i=0}^n a_i = d$.
\begin{equation}
\sum_{i=0}^n \del_{x_i}(f)x_i
= \sum_{i=0}^n \begin{Bmatrix} \left(\prod_{j\neq i} x_j^{a_j}\right) a_i x_i^{a_i-1} x_i, & \text{ for } a_i > 0
\\ 0, &\text{ for } a_i = 0 \end{Bmatrix}
= \sum_{i=0}^n a_i f = df
\end{equation}
\end{proof}

\begin{corollary}
$\V(\del_{x_0}(f),..\del_{x_n}(f), df) = \V(\del_{x_0}(f),..\del_{x_n}(f))$
\end{corollary}


\begin{lemma}
Let $\del_1,\del_2$ be partial derivatives, then $\del_1.\del_2 = \del_2.\del_1$.
\end{lemma}
\begin{proof}
We only need to prove this for monomials, and we'll perform an induction on the degree.
If $f$ is a monomial of degree less than 2, then $\del_1(\del_2(f)) = 0 = \del_2(\del_1(f))$. 
Now suppose $f = x_if'$ and $\del_1(\del_2(f')) = \del_2(\del_1(f'))$.

\begin{equation}
\del_1.\del_2(x_if') = \del_1(x\del_2(f') + f'\del_2(x)) = 
\del_1(x)\del_2(f') + \del_1(f')\del_2(x) + x\del_1(\del_2(f')) + \underset{=0}{\underbrace{f'\del_1(\del_2(x))}}
\end{equation}
The last term is symmetric in $\del_1,\del_2$ (by assumption).
\end{proof}

\begin{corollary}[Taylor's formula]
Let $f \in k[x_0,..x_n]$ be a polynomial and $f = \sum_{i=0}^d f_i$ its decomposition into homogeneous parts.
Then $f_i = \frac{1}{i!} \sum_{|\bf{\alpha}|=i} \del^{\bf{\alpha}}(f)(0)x^{\bf{\alpha}}$ in multi-index notation.
\end{corollary}
\begin{proof}
% TODO ...
\end{proof}


