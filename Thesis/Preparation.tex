\section{General facts}

\subsection{projective space}

In order to do geometry we need to explain what points, lines, surfaces are, and this can be done in several ways, synthetically or analytically.

There are several formulations of projective space, with varying degree of generality, and for our purposes I have chosen the one in terms of homogeneous coordinates.

Let $k$ be an algebraically closed field and $n$ a natural number.
The projective space $\proj^n_k$ is a topological space given as set by  $k^{n+1} - \{ 0 \}$ modulo the relation $X=(x_0,..x_n) \sim Y=(y_0,..y_n)$ if and only if $X$ and $Y$ are linearly dependent (as vectors in $k^{n+1}$). The equivalence class of $X$ will be denoted $[x_0:x_1:..:x_n]$.

The topology is given by the closed subsets
\begin{equation}
\V(I) =
\mkset{ [x_0:..x_n] \in \proj^n_k}
      { \forall f\in I \text{ homogeneous}, f(x_0,..x_n) = 0}
\end{equation}
for homogeneous ideals $I$ of the polynomial ring $k[x_0,..x_n]$.
The topology is known as Zariski's topology.
Because $k[x_0,..x_n]$ is a Noetherian ring, every ideal is finitely generated and in particular every homogeneous ideal is finitely generated by homogeneous elements.
So we may say that a closed set consists precisely of the points in projective space where a finite set of homogeneous ideals vanish.

In this framework I want to identify geometric objects with homogeneous ideals of $k[x_0,..x_n]$.
However, most of the time it is easier to speak of the closed subsets of projective space, instead of the ideal $I$, and $I$ itself can be recovered by means of Hilberts's Nullstellensatz in the form of its radical.

A \emph{hypersurface} is given by one equation $f=0$ for $f\in k[x_0,..x_n]$, and its set of points is $\V(f)$.
In case of $f$ being a linear form we call $\V(f)$ a \emph{hyperplane}.

A line is determined by $n-1$ $k$-linearly independent linear forms and a point by $n$ $k$-linearly independent linear forms.

\subsection{partial derivatives}

In this section we discuss partial derivatives -- a useful algebraic tool to obtain some geometric properties.
 % TODO

\begin{definition}[partial derivative]
Let $D : k[x_0,..x_n] \to k[x_0,..x_n]$ be a derivation over $k$, i.e. a homomorphism of $k$-modules which satisfies the Leibniz rule $D(xy) = xDy+yDx$ for $x,y \in k[x_0,..x_n]$. Furthermore let $D(h) \in k$ for every linear form $h$. We call such a derivation a \emph{partial derivative}.
\end{definition}

\begin{example}
The standard example for partial derivatives are of course the partial derivates with respect to one of the variables $x_i$, defined as $\del_{x_i}(x_j) = \delta_{i,j} := \begin{cases} 1, & \text{ for } i = j \\ 0, & \text {otherwise.} \end{cases}$
\end{example}

\begin{example}
For any monomial $\{ h_i\}_{i=0}^n$ basis of $\bigoplus_{i=0}^n kx_i \subset k[x_0,..x_n]$ we obtain a family of partial derivatives $\{ \del_{h_i} \}_{i=0}^n$ for which $\del_{h_i}(h_j) = \delta_{i,j}$ holds. The construction goes as follows: Let $M = (a_{i,j})  \in k^{(n+1)\times(n+1)}$ be the base change matrix and $\widetilde M = (\widetilde a_{i,j})$ be its inverse.
This just means $h_i = \sum_{j=0}^n a_{i,j} x_j$ and hence $\del_{x_k}(h_i) = a_{i,k}$.
From $\delta_{i,j} = \sum_{k=0}^n a_{i,k}\widetilde a_{k,j}
= \sum_{k=0}^n \del_{x_k}(h_i) \widetilde a_{k,j}$ it is obvious that we need to define

\begin{equation}
\del_{h_j} = \sum_{k=0}^n \widetilde a_{k,j} \del_{x_k}
\end{equation}

Another write to write this would be
\begin{equation}
\begin{pmatrix} \del_{h_0} \\ \vdots \\ \del_{h_n} \end{pmatrix}
= \widetilde M^T
\begin{pmatrix} \del_{x_0} \\ \vdots \\ \del_{x_n} \end{pmatrix}
\end{equation}
\end{example}

A useful fact, that allows us to recover a homogeneous polynomial by its partial derivatives is

\begin{proposition}[Euler's formula]
For any $f \in k[x_0,..x_n]$ homogeneous of degree $d$ we have the equality
\[ df = \sum_{i=0}^n \del_{x_i}(f)x_i \]
\end{proposition}
\begin{proof}
By linearity we only need to prove the monomial case $f = \prod_{i=0}^n x_i^{a_i}$, $a_i$ being integers such that $\sum_{i=0}^n a_i = d$.
\begin{equation}
\sum_{i=0}^n \del_{x_i}(f)x_i
= \sum_{i=0}^n \begin{Bmatrix} \left(\prod_{j\neq i} x_j^{a_j}\right) a_i x_i^{a_i-1} x_i, & \text{ for } a_i > 0
\\ 0, &\text{ for } a_i = 0 \end{Bmatrix}
= \sum_{i=0}^n a_i f = df
\end{equation}
\end{proof}

\begin{corollary}
$\V(\del_{x_0}(f),..\del_{x_n}(f), f) = \V(\del_{x_0}(f),..\del_{x_n}(f))$
\end{corollary}


\begin{lemma}
Let $\del_1,\del_2$ be partial derivatives, then $\del_1.\del_2 = \del_2.\del_1$.
\end{lemma}
\begin{proof}
We only need to prove this for monomials, and we'll perform an induction on the degree.
If $f$ is a monomial of degree less than 2, then $\del_1(\del_2(f)) = 0 = \del_2(\del_1(f))$. 
Now suppose $f = x_if'$ and $\del_1(\del_2(f')) = \del_2(\del_1(f'))$.

\begin{equation}
\del_1.\del_2(x_if') = \del_1(x\del_2(f') + f'\del_2(x)) = 
\del_1(x)\del_2(f') + \del_1(f')\del_2(x) + x\del_1(\del_2(f')) + \underset{=0}{\underbrace{f'\del_1(\del_2(x))}}
\end{equation}
The last term is symmetric in $\del_1,\del_2$ (by assumption).
\end{proof}

\begin{corollary}[Taylor's formula]
Let $f \in k[x_0,..x_n]$ be a polynomial and $f = \sum_{i=0}^d f_i$ its decomposition into homogeneous parts.
Then $f_i = \frac{1}{i!} \sum_{|\bf{\alpha}|=i} \del^{\bf{\alpha}}(f)(0)x^{\bf{\alpha}}$ in multi-index notation.
\end{corollary}
\begin{proof}
% TODO ...
\end{proof}





\subsection{singularities and the tangent plane}

% TODO


\subsubsection{Invariance of singularities under a change of the system of partial derivatives}

Oftentimes it is more convenient to perform base changes to simplify equations and sometimes one can do without, bnut in exchange one needs to introduce more flexible tools.

Remember, in order to study singularities we introduced partial derivatives $\del_{x_i} : k[x_0,..x_n] \to k[x_0,..x_n]$ which satisfied $\del_{x_i}(x_j) = \delta_{i,j}$.

%TODO
TODO: Define partial derivative and prove that they are a $k$-vector space spanned by the $\del_{x_i}$.
Then introduce base change.


\subsection{plane conics -- the question of degeneracy}

A plane conic is a algebraic variety $\V(f)$ given by a quadratic form $f \in k[x_0,x_1,x_2]$. One might ask the question whether the conic is a union of two lines (in which case the conic is called \emph{degenerate}), or in algebraic terms, whether $f$ factors into two linear forms or whether it is irreducible.
Let's turn our attention the an easier question: When is a conic singular?

Assume that the characteristic of our base field $k$ is not 2, then the conic can be written, for appropriate coefficients $a,b,c,d,e,f$ as:
\begin{equation}
f = ax_0^2 + 2bx_0x_1 + cx_1^2 + 2dx_0x_2 + 2ex_1x_2 + fx_2^2
\end{equation}
The singular points are given by the system of equations
\begin{equation}
\del_{x_0} f = \del_{x_1} f = \del_{x_2} f = 0
\end{equation}
which written out in matrix notation, amounts to
\begin{align}
2
\underset{=:M}{\underbrace{
\begin{pmatrix}
a & b & e \\
b & c & d \\
e & d & f
\end{pmatrix}
}}
\begin{pmatrix}
x_0 \\ x_1 \\ x_2
\end{pmatrix}
=
0
\end{align}
We call the matrix $M$.
A singular point $[s_0:s_1:s_2] \in \proj^2_k$ would of couse be a non-zero solution of above equation and as such can only exist precisely if the determinant of $M$ vanishes.
So far we have obtained

\begin{corollary}
Let $k$ be a field of characteristic not 2 and $f \in k[x_0,x_1,x_2]$ be a quadratic form. The conic $\V(f) \subset \proj^2_k$ is singular if and only if
\begin{equation}
\det
\begin{pmatrix}
a & b & e \\
b & c & d \\
e & d & f
\end{pmatrix}
= acf + 2bde - ce^2 - ad^2 - b^2f = 0
\end{equation}
\end{corollary}

Returning to our initial question we want to establish the fact that the conic given by the quadratic form $f$ is irreducible if and only if it is non-singular.
For that assume reducibility, that is $f = gh$ for 1-forms $g$ and $h$. Then $\del_{x_i}f = H\alpha + \beta G$ for $\alpha := \del_{x_i}G \in k$ and $\beta := \del_{x_i}H \in k$.
Assume now that a point $P$ lies in the intersection of $\V(H)$ and $V(G)$, then $\del_{x_i}f(P) = H(P)\alpha + \beta G(P) = 0$, so the intersection is a singularity. Of course, in the projective plane there always exists an intersection for any two lines.
(This is just a consequence of linear algebra: Say $H = a_0x_0 + a_1x_1 + a_2x_2, G = b_0x_0 + b_1x_1 + b_2x_2$, then the kernel of the matrix $\begin{pmatrix} a_0 & a_1 & a_2 \\ b_0 & b_1 & b_2 \end{pmatrix}$ is nontrivial and if $(s_0,s_1,s_2) \neq 0 \in k^3$ lies in the kernel, then $P := [s_0:s_1:s_2]$ lies in the intersection.)

The converse can be seen as follows. Let $P=[p_0:p_1:p_2]$ be a singularity and $P'=[p'_0:p'_1:p'_2]$ is any other point on the conic (for instance any intersection point of $\V(f) \cap \V(x_i)$). For $f$ to contain the line through $P$ and $P'$ means that $f(\lambda P + \mu P') = 0 \in k[\lambda,\mu]$.

Again, Euler's equality shows itself to be quite useful in the calculation
\begin{align}
0
= 2f(\lambda P')
=& \sum_{i=0}^2 \lambda p'_i \del_{x_i}f(\lambda P') 
+ \sum_{i=0}^2 \lambda p'_i \underset{=0}{\underbrace{\del_{x_i}f(\mu P)}}
\\
=& \sum_{i=0}^2 \lambda p'_i \del_{x_i} f(\lambda p'+\mu p) 
\\
=& 2f(\lambda p' + \mu p') - \sum_{i=0}^2 \mu p_i \del_{x_i}f(\lambda P' + \mu P)
\end{align}

Finally I claim that the last sum disappears due to the equality
\begin{equation}
\sum_{i=0}^2 p_i \del_{x_i}f = 0 \in k[x_0,x_1,x_2]
\end{equation}

The calculation is straight-forward:
\begin{align}
\sum_{i=0}^2 p_i \del_{x_i} f
=& \sum_{i=0}^2 p_i \sum_{j=0}^2 \del_{x_j}\del_{x_i}fx_j
\\
\overset{\text{rearrange}}=& \sum_{j=0}^2 x_j \sum_{i=0}^2 \del_{x_i}\del_{x_j}fp_i = \sum_{j=0}^2 x_j \del_{x_j}f(P) = 0
\end{align}

Now that we have shown the sum to disappear, we obtain $0 = 2f(\lambda P' + \mu P)$ (in characteristic not 2), hence the conic contains a line. This proves

\begin{theorem}
Let $k$ be a field of characteristic not 2 and $\V(f) \subset \proj^2_k$ a conic given by a quadratic form $f = ax_0^2 + 2bx_0x_1 + cx_1^2 + 2dx_0x_2 + 2ex_1x_2 + fx_2^2$.
Then the following are equivalent:
\begin{enumerate}
\item The conic is degenerate.
\item The quadratic form $f$ factors into two linear forms, $f=gh$.
\item $\V(f)$ is a union of two lines.
\item $\V(f)$ is singular.
\item $\det
\begin{pmatrix}
a & b & e \\
b & c & d \\
e & d & f
\end{pmatrix}
= acf + 2bde - ce^2 - ad^2 - b^2f = 0$
\end{enumerate}
\end{theorem}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{the restriction of equations}


One useful operation one may perform is to take the intersection of a hypersurface $S = \V(f) \subset \proj^n_k$ with a hyperplane $\Pi = \V(h)$. Now one expects from geometric intuition that the $d$-form $f$ `restricts' to a $d$-form on $\Pi$ where we think of $\Pi$ as $\proj^{n-1}_k$, unless $S$ contains $\Pi$ in which case one would expect the equation to `restrict' to the zero polynomial.
The goal of this section is to make this intuition precise, by defining a homomorphism $\var{res} : k[x_0,..x_n] \to k[x_0,..x_{n-1}]$ and an isomorphism $\proj^{n-1} \overset\sim\to \Pi \hookrightarrow \proj^n$.

First we may assume without loss of generality that
$h = \alpha x_n + \widetilde h$ where $0 \neq \alpha \in k$ and $\widetilde h \in k[x_0,..x_{n-1}]$.

Then we define
\begin{equation}
\var{res} : \begin{cases}
x_i \mapsto x_i, & \text{for } i \neq n \\
x_n \mapsto -\frac 1\alpha \widetilde h. &
\end{cases}
\end{equation}

and furthermore we extend it to

\begin{equation}
\widetilde{\var{res}} : k[x_0,.. x_n] \overset{\var{res}}\to k[x_0,.. x_{n-1}] \hookr k[x_0,.. x_n]
\end{equation}

The isomorphism $\vartheta : \proj^{n-1}_k \to \Pi$ we define as
\begin{equation}
\vartheta : [x_0:..:x_{n-1}] \mapsto [x_0:..:x_{n-1}:-\frac 1\alpha \widetilde h(x_0,..,x_{n-1})]
\end{equation}
One can easily see by evaluation that $h$ vanishes on the image of $\vartheta$, so $\vartheta$ maps into $\Pi$. To confirm, that we indeed defined an isomorphism we construct an inverse.
A left-inverse of couse has to look like this:
\begin{equation}
\vartheta^{-1} : [x_0:..:x_n] \mapsto [x_0:..:x_{n-1}]
\end{equation}
To show well-definedness, one needs to prove that $[x_0:..:x_{n-1}] = [0:...:0]$ is not in the image. Suppose it were, then $0 = h(x_0,..,x_{n-1}) = \alpha x_n + \widetilde h(0)$ and therefore $x_n = -\frac 1\alpha \widetilde h(0) = 0$ as well, so the preimage would have to be $[x_0:..:x_n] = [0:..:0]$ which cannot happen.
What we've also seen in the calculation so far is that the coordinate $x_n$ depends uniquely on the other ones, hence $\vartheta^{-1}$ as defined above is a right-inverse.

Having everything defined we obtain the following relation of a form $f$ and its restriction to the plane $\var{res}(f)$: Let $P = [p_0:..:p_n] \in \Pi$ be a point on the plane, so $p_n = -\frac 1\alpha \widetilde h(p_0,..,p_{n-1}) = -\frac 1\alpha \widetilde h(\vartheta^{-1}(P))$.

\begin{align}
f(P) = 0
&\text{ iff } f(p_0,..,p_{n-1},-\frac 1\alpha \widetilde h(\vartheta^{-1}(P))) = 0
\\
&\text { iff } \var{res}(f)(\vartheta^{-1}(P)) = 0
\\
(&\text{ iff } \widetilde{\var{res}}(f)(P) = 0)
\end{align}

This allows us to understand the projective variety $\Pi \cap \V(f)$ in terms of the equation $\var{res}(f) = 0$ by $\Pi \cap \V(f) \simeq \V(\var{res}(f)) \subset \proj^{n-1}_k$.
Iterating this process we may consider further restrictions to $\proj^{n-2}_k$ etc.

Another thing I want to point out is that the endomorphism $\widetilde{\var{res}}$ is idempotent (i.e. $\widetilde{\var{res}}.\widetilde{\var{res}} = \widetilde{\var{res}}$) and the kernel is the ideal generated by $h$: On one hand $\var{res}(h) = 0$, on the other hand if $\var{res}$ maps a form $f$ to $0$, then $f$ vanishes on $\Pi$, so $h$ divides $f$ (say, by Hilbert's Nullstellensatz).

In particular we obtain
\begin{proposition}
Let $k$ be algebraically closed.
For any $d$-form $f \in k[x_0,..,x_n]$ and $\widetilde{\var{res}}$ as defined before there exists a ($d-1$)-form $\widetilde f$ such that
\begin{equation}
f = \widetilde{\var{res}}(f) +  h\widetilde f
\end{equation}
\end{proposition}

\subsubsection{the coordinate ring view}


% TODO

We can formulate the result in a slightly higher level of generality. So first we replace algebraic subsets of $\proj^n_k$ by ideals $J \subset k[x_0,..x_n]$.
Furthermore, algebraic subsets of $J = \V(h)$ correspond to ideals $I \subset k[x_0,..x_n]/J$.


\subsection{lines on surfaces}

% TODO: through two points goes a line
% TODO: How to see if a line through two points lies on a surface

So far we have defined a line to be an intersection of hyperplanes, but of course we also want to understand a line as the unique intersection of such hyperplanes containing two distinct points.

Let's fix a projective space $\proj^n_k$ and let $P=[p_0:..p_n]$ and $Q=[q_0:..q_n]$ be distinct points in $\proj^n_k$. Now consider the homomorphism
\begin{equation}
L : \begin{cases}
k[x_0,..x_n] &\to k[\lambda,\mu] \\
f &\mapsto f(\lambda P + \mu Q) := f(\lambda p_0 + \mu q_0, .. \lambda p_n + \mu q_n)
\end{cases}
\end{equation}

Its kernel contains those polynomials, which vanish on $\lambda P + \mu Q$ and in particular on any point $\lambda_0 P + \mu_0 Q$ for $[\lambda_0:\mu_0] \in \proj^1_k$, such as $P$ and $Q$.
These were the points we expected to be contained on the line anyhow.
Consider the linear map $\bigoplus_{i=0}^n kx_i \to k\lambda \oplus k\mu$ defined by the $2\times (n+1)$-matrix
\begin{equation}
M=
\begin{pmatrix}
p_0 & \ldots & p_n \\
q_0 & \ldots & q_n
\end{pmatrix}
\end{equation}

Because $P$ and $Q$ are distinct points in projective space, the matrix has full rank 2, and hence the kernel is spanned by $(n-1)$ linear forms $h_0,..h_{n-2}$.
By our choice of the matrix $M$, these linear forms lie in the kernel of $L$: $h_i(\lambda P + \mu Q) = h_i(P) \lambda + h_i(Q)\mu = 0 \lambda + 0 \mu$.
In fact, the kernel of $L$ is spanned by these linear forms already, which follows from the \emph{projective Nullstellensatz} which I will just mention here without proof:

\begin{theorem}[Projective Nullstellensatz]
% TODO
\end{theorem}

Assume now that $f \in \ker(L)$, then $\V(f) \supset \V(h_0,..h_{n-2})$ and hence $(f) \subset \sqrt{(h_0,..h_n)}$.

% TODO
\begin{todo}
\item Show $I = (h_0,..h_{n-2})$ is radical because the quotient ring $k[x_0,..x_n]/I$ is iso to $k[y_0,y_1]$ and hence reduced (use the restriction homomorphisms)
\item Show that $\V(h_0,..h_{n-2}) = \{ \lambda P + \mu Q \}$
\end{todo}

